## 模型评估与选择

#### 模型评估方法
划分训练集和数据集的几种方法。

1. 留出法
    直接将数据集D划分为两个互斥的集合，一个作为训练集，一个作为测试集。一般将数据集D的2/3~4/5用作训练集，剩余的用作测试集。
划分原则：
* 如何保持数据分布的一致性? 分层采样：保留类别比例的采样方式
* 存在问题：单次留出法的得到的估计结果不够稳定可靠，一般采用若干次随机划分、重复进行试验评估后取平均值作为留出法的评估结果  
缺陷：训练集过大，更接近整个数据集，但是由于测试集较小，导致评估结果缺乏稳定性；测试集大了，偏离整个数据集，与根据数据集训练出的模型差距较大，缺乏保真性。

2. 交叉验证法
    !["K则交叉验证"](https://raw.githubusercontent.com/Joey-Hu/markdown-noteook/master/machine_learning/images/model_evaluation_and_selection/k-fold%20cross-validation.png)
    将数据集划分为k个大小相似的互斥子集，每个子集轮流做测试集，其余做训练集，最终返回这k个训练结果的均值
    交叉验证法的结果的稳定性和保真性很大程度上取决于k取值，“k折交叉验证”
    与留出法相似，将数据集D划分为k个子集同样存在多种划分方式，为减小因样本的划分不同而引入的误差，k折交叉验证通常要随机使用不同的划分重复p次，最终结果是这p次k折交叉验证结果的均值。

3. 自助法
    包含m个样本的数据集D，对D进行随机抽样构建D'（抽取m次）,然后将抽到的数据对象放回D，D'用作训练集，剩余数据作为测试集，原数据集里的某些元组可能在样本中出现多次。一个样本从未被选中的概率是$(1-\frac{1}{m})^m$，当m很大时，该式子近似等于0.368。
    
#### 性能度量方法

**混淆矩阵**
!["confusion matrix"]()

**评价度量**
错误率 $err(f,D)=\frac{FN + FP}{TP + FN + FP + TN}$
准确率 $acc(f,D)=\frac{TP + TN}{TP + FN + FP + TN}$
精度(precision, P)/查准率 $precision =\frac{TP}{TP + FP} $  -- 纵向
召回率(recall, R)/查全率 $recall = \frac{TP}{TP + FN}$ -- 横向

!["P-R曲线"]()
绘制方法：
1. 根据分类器给所有样本一个分为正例的概率
2. 按照概率将样本从大到小排序
3. 对排序后的数据集进行切分，左边的样本当正例，右边的样本当反 例，然后计算R和P。从数据集左端开始切分，每切 一刀，计算一次R和P，切m+1次 就可得出P-R曲线

P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。

$F_1$度量
P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的调和平均
$$
\begin{equation}
F_1 = \frac{2*precision*recall}{precision+recal}
\end{equation}. \tag{1}
$$

ROC和AUC
如果分类器给出的预测结果是一个实数值或概率，我们通常给定一个阈值，大于阈值的定为正例，小于阈值的定为反例，如logistic回归或神经网络等。
如果我们像绘制P-R曲线时根据预测结果从大到小把样本排序，但重新定义两个变量真正例率TPR，假正例率FPR
$$
\begin{equation}
TPR = \frac{TP}{TP + FN}
\end{equation}. \tag{2}
$$
$$
\begin{equation}
FPR = \frac{FP}{FP + TN}
\end{equation}. \tag{3}
$$

ROC曲线的绘制：
1. 根据预测结果从大到小把样本排序
2. 将阈值从大到小调节，每更新一次阈值，计算一次TPR和FPR
3. 绘制得到的所有点对（TPR,FPR）

!["roc_auc"]()

提高模型准确率的技术：
袋装(bagging)：使用多个模型对数据集进行自助样本，训练模型
boosting和AdaBoost
随机森林






参考：
https://blog.csdn.net/donger__chen/article/details/80989923